---
title: "p8105_hw5_zl3119"
author: "Zheyan"
date: "11/11/2021"
output: github_document
---

```{r, include = FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "95%"
)

theme_set(theme_minimal() + theme(legend.position = 'bottom'))

options(
  ggplot2.continuous.colour = 'viridis',
  ggplot2.continuous.fill = 'viridis'
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
# Problem 1

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository Describe the raw data. Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
homicide_df = 
  read_csv('homicide-data.csv', show_col_types = FALSE, na = c("", 'Unknown')) %>% 
  mutate(city_state = str_c(city, state),
         resolution = case_when(
           disposition == 'Closed without arrest' ~ 'unsolved',
           disposition == 'Open/No arrest' ~ 'unsolved',
           disposition == 'Closed by arrest' ~ 'solved'
         )) %>% 
  relocate(city_state)

```


For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == 'BaltimoreMD')

baltimore_summary = 
  baltimore_df %>% 
    summarise(
      unsolved = sum(resolution == 'unsolved'),
      n = n()
    )

baltimore_test = 
  prop.test(
    x = baltimore_summary %>% pull(unsolved),
    n = baltimore_summary %>% pull(n)
  )

baltimore_test %>% 
  broom::tidy() %>% 
  knitr::kable()


```


Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
prop_test_function = function(city_df){
  
  city_summary = 
    city_df %>% 
      summarise(
        unsolved = sum(resolution == 'unsolved'),
        n = n()
      )
  
  city_test = 
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n)
    )
  
  city_test
}


# prop_test_function(baltimore_df)

# check another city
homicide_df %>% 
  filter(city_state == 'AlbuquerqueNM') %>% 
  prop_test_function() %>% 
  broom::tidy() %>% 
  knitr::kable()

```
Iterate across all cities

```{r, warning=FALSE}
results_df = 
  homicide_df %>% 
    nest(-city_state) %>% 
    mutate(
      test_results = map(data, prop_test_function),
      tidy_results = map(test_results, broom::tidy)
    ) %>% 
    select(city_state, tidy_results) %>% 
    unnest(tidy_results) %>% 
    select(city_state, estimate, starts_with('conf'))

head(results_df) %>% 
  knitr::kable()
```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.


```{r}
results_df %>% 
  filter(city_state != 'TulsaAL') %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  

``` 



# Problem 2

Data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

### Create and tidy data

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time; Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary


```{r Read data, warning=FALSE, message=FALSE}
data_path = 'data'
file_list = list.files(data_path)


extract_ID_arm = function(filename) {
    filefront = strsplit(filename, split = '.', fixed = TRUE)[[1]][1]
    # extract info about arm and ID
    arm = strsplit(filefront, split = '_', fixed = TRUE)[[1]][1]
    ID = as.integer(strsplit(filefront, split = '_', fixed = TRUE)[[1]][2])
    # read file
    df_temp = read_delim(paste(data_path, filename, sep = '/'), delim = ',')
    df_temp %>%  
         mutate(arm = arm,
         ID = ID) 
}


df = map_df(list.files(data_path), extract_ID_arm) %>% 
  arrange(arm, ID) %>% 
  relocate(arm, ID) %>% 
  pivot_longer(week_1:week_8, names_to = 'week', names_prefix = 'week_',values_to = 'value') %>% 
  mutate(week = as.factor(week))

# show data
head(df,16) %>% 
  knitr::kable()

```

### Visualization

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
df %>% 
  ggplot(aes(x = week, y = value, group = paste0(ID, arm), color = arm)) +
  geom_line()
```


Two groups are similar at the first week. Paticipants in the experiment group have increasing value while those from control group have fluctuate values. Paticipants from  experiment grouphave higher value at last two weeks.

# Problem 3

```{r generate missing data, include = FALSE}
set.seed(10)

iris_with_missing = iris %>% 
  # every column has 20 missing values
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```


For numeric variables, fill in missing values with the mean of non-missing values
For character variables, fill in missing values with "virginica"

```{r}
# impute function
impute_func = function(df, col) {
  colvalues = df %>% pull(col)
  if (class(colvalues) == 'numeric') {
    imputed = mean(colvalues, na.rm = TRUE)
    colvalues = colvalues %>% replace_na(imputed)
  } 
  else if (class(colvalues) == 'character') {
    colvalues = colvalues %>% replace_na('virginica')
  }
  colvalues
}

# copy an iris set to impute on
iris_imputed = iris_with_missing
# impute
for (col in colnames(iris_with_missing)) {
  iris_imputed[col] = impute_func(iris_imputed, col)
}

# check if still has na
print(paste0('Missing value counts:',sum(is.na(iris_imputed))))

# show imputed dataframe
head(iris_imputed,12) %>% 
  knitr::kable()
```






